---
title: "sentiment_detection3"
author: "Frederick"
date: "6/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Sentiment Analysis
* Text Transcriptions (GCP) of Kantar Media ads
* POS and NER to remove words not useful to sentiment
* Create corpus of lemmatized words
* Run Lasso Regression (Naive Bayes, SVM) with target var = ad_tone 
  * ad_tone: 1=Contrast, 2=Promote, 3=Attack, 4=Other, please explain (option 4 - Fb & state leg coding only)





```{r}
library(tidyverse)
library(tidytext)
library(SnowballC)

error_msgs <- "(INVALID AUDIO FILE PATH|NEEDS LONG-RUNNING)"
df <- read_csv("../data/tv-ads-kantar-meta/tv_ads_transcribed.csv") %>% 
  distinct(id, tone, .keep_all = TRUE) %>% 
  filter(str_detect(transcript, "[A-Za-z]")) %>% 
  filter(!str_detect(transcript, error_msgs)) 

write_csv(df, "../data/tv-ads-kantar-meta/tv_ads_transcribed_clean.csv")
```

```{r}

words_clean <- df %>% 
  unnest_tokens(word, transcript) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(word = wordStem(word)) 

# got rid of 3 ads w/ just stop words contained (check with anti)
df <- df %>% 
  semi_join(words_clean, by = 'id')

# Words appear at least N times
words_filtered <- words_clean %>% 
  add_count(word) %>% 
  filter(n > 70) 

```

# Graphs on words counts

```{r}
words_clean %>% 
  count(word) %>% 
  filter(n > 5) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 20) +
  scale_x_log10() + 
  labs(title = "Distribution of top words")


words_clean %>% 
  count(word) %>% 
  top_n(20, n) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Most common words in Kantar Ads",
       subtitle = "Words from GCP were stemmed",
       x = "",
       y = "")
  
words_clean %>% 
  count(tone, word) %>% 
  group_by(tone) %>% 
  top_n(20, n) %>% 
  ungroup() %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n, fill = tone)) +
  geom_col() +
  coord_flip() + 
  facet_wrap(~ tone) + 
  theme(legend.position = "none") + 
  labs(title = "Most common words in Kantar Ads by Tone",
       subtitle = "Words from GCP were stemmed; tone from WMP coders",
       x = "", 
       y = "")



  
```


# Network graph
```{r}
library(widyr)
library(igraph)
library(ggraph)



top_word_cors <- words_filtered %>% 
  select(id, word) %>% 
  pairwise_cor(word, id, sort = TRUE) %>% 
  head(200)


calculate_mode <- function(x) {
  uniqx <- unique(x)
  uniqx[which.max(tabulate(match(x, uniqx)))]
}


words_meta <- words_filtered %>% 
  filter(word %in% top_word_cors$item1 |
           word %in% top_word_cors$item2) %>% 
  group_by(word) %>% 
  summarise(tone = calculate_mode(tone), 
            n = n())

set.seed(1234)
top_word_cors %>% 
  graph_from_data_frame(vertices = words_meta) %>% 
  ggraph() +
  geom_edge_link() +
  geom_node_point(aes(size = n,
                      color = tone)) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Words used to attack/promote candidates",
       subtitle = "Kantar Ads; vertices are correlation between words",
       size = "# of Ads",
       color = "Tone (WMP)")

```




# Predictive Model

```{r}
library(caret)
library(glmnet)
library(Matrix)
library(e1071)

X <- words_filtered %>% 
  cast_sparse(id, word)

# row = ad; column = word
dim(X) 

y <- ifelse(df$tone[match(rownames(X), df$id)] == "attack", 0, 1)
```

## Lasso Regression
```{r}
library(broom)
library(glmnet)

lasso_model <- glmnet(X, y)


## Highest impact words
strong_words <- lasso_model %>% 
  tidy %>% 
  filter(term != "(Intercept)", step %in% 1:12) %>% 
  count(term) %>% 
  pull(term) 
strong_words

lasso_model %>% 
  tidy() %>% 
  filter(term %in% strong_words) %>% 
  ggplot(aes(lambda, estimate, color = term)) +
  geom_line() +
  geom_hline(yintercept = 0, lty = 2) + 
  scale_x_log10() + 
  labs(title = "Most Impactful Words",
       subtitle = "As Lambda Inc. Penalty for Large Coefficient increases (first 25 steps)")


lasso_model %>% 
  tidy %>% 
  count(lambda) %>% 
  ggplot(aes(lambda, n)) +
  geom_line() + 
  scale_x_log10() +
  labs(title = "How many words do we include in our political lexion?",
       subtitle = "Need to find appropriate lambda for balance in no. of words + predictive power")



```

## Cross-validated lasso regression
```{r}
cv_lasso_model <- cv.glmnet(X, y, family = 'binomial')
plot(cv_lasso_model)


cv_lasso_model$glmnet.fit %>% 
  tidy() %>% 
  filter(lambda == cv_lasso_model$lambda.1se,
         term != "(Intercept)") %>% 
  top_n(30, abs(estimate)) %>% 
  mutate(direction = if_else(estimate >= 0, "promote", "attack"),
         term = fct_reorder(term, estimate)) %>%
  ggplot(aes(term, estimate, fill = direction)) +
  geom_col() + 
  theme(legend.position = "none") + 
  coord_flip() +
  labs(title = "Which words are used to promote/attack a candidate?",
       subtitle = "Lasso binomial regression on GCP Transcripts + WMP ad_tone",
       x = "",
       y = "Regression Coefficient")

ggsave("../output/kantar_words_reg.png", width = 7, height = 4)
```


## Validation
```{r}
single_cv_lasso <- glmnet(X, y, lambda = cv_lasso_model$lambda.1se, family = "binomial")
pred <- predict(single_cv_lasso, X, type = 'response')

pred <- as.numeric(ifelse(pred >= 0.5, 1, 0))
xtab <- table(pred, y)
confusionMatrix(xtab)


```





# Manually referring back to original transcripts
```{r}


see_articles <- function(word_of_interest) {
  my_select <- df %>% 
    unnest_tokens(word, transcript) %>% 
    anti_join(stop_words, by = "word") %>% 
    mutate(stem = wordStem(word)) %>% 
    filter(stem == word_of_interest) %>% 
    pull(id)


  transcripts <- df %>% 
    filter(id %in% my_select) %>% 
    pull(transcript)
  
  return(transcripts)
}

see_articles("balanc")
see_articles("vote")
see_articles("solut")
```





## Appendix: 

Naive bayes

```{r}
ad_word_df <- as.data.frame(as.matrix(X))
ad_word_df$tone <- ifelse(df$tone[match(rownames(X), df$id)] == "attack", 0, 1)
table(ad_word_df$tone)

set.seed(1234)
inTraining <- createDataPartition(ad_word_df$tone, p = .75, list = FALSE)

train <- ad_word_df[inTraining,]
test <- ad_word_df[-inTraining,]

set.seed(1234)
fit <- naiveBayes(tone ~ ., train, laplace = 1)

a <- as.list(fit)
length(a$tables)
names(a$tables)
a$tables$contributor


pred <- predict(fit, test %>% select(-tone))
confusionMatrix(pred, test$tone)

# fitControl <- trainControl(method = "none")
# 
# fit <- train(tone ~ .,
#              train,
#              method = 'nb',
#              trControl = fitControl,
#              tuneGrid = data.frame(fL = 1, usekernel = FALSE, adjust = 1.5))



```

